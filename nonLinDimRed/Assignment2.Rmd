---
title: "Assignment 2"
subtitle: "PCA and Principal Curves"
author: "Alexander, Ulrik, Hannes"
date: "`r format(Sys.time(), '%d/%b/%Y')`"
output: 
   html_document:
    code_folding: hide
    theme: cerulean
    highlight: textmate
    number_sections: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>")
# setwd("/home/ajo/gitRepos/ML/nonLinDimRed")
library(sm)
library(ggplot2)
library(dplyr)
library(cluster)
library(fpc)
library(princurve)
```

# PCA and Principal Curves for ZIP numbers

Consider the ZIP data set. Read the train data set and select only the zeros. 

```{r}
zip.train <- read.table("../unsup/zip.train")
dim(zip.train)
table(zip.train[,"V1"])
zip.zeros <- zip.train %>% dplyr::filter(V1 == 0)
dim(zip.zeros)
```

## Questions

### Hierarchical Clustering

Hierarchical clustering using the **ward.D** method. The dendrogram is plotted below and it is cut into $k = 4$ clusters. 

```{r}
hcl1 <- hclust(dist(zip.zeros), method="ward.D") # Euclidean distance, ward.D method. 
plot(hcl1)
clusts <- cutree(hcl1, 4)
table(clusts)
zip.zeros$cluster <- clusts # Add the cluster of each point to a new column. 
```

The table above shows how the zeros are distributed across the clusters. 

### Average Digit at Each Cluster

The average digit in each cluster is plotted. 

```{r}
# Function for plotting a digit. Taken from "PCA_ZIP_numbers.Rmd" (lectures).
plot.zip <- function(x,use.first=FALSE){
  x<-as.numeric(x)
  if (use.first){
    x.mat <- matrix(x,16,16)
  }else{
    x.mat <- matrix(x[-1],16,16)
  }
  image(1:16,1:16,x.mat[,16:1],
        col=gray(seq(1,0,l=12)))
  if (!use.first) title(x[1])
}

# Calculate the average digit in each cluster.
# This could be probably be done more efficiently
average.images <- zip.zeros[1:4,-1]
for (i in 1:4){
   for (j in 1:256){
      average.images[i,j] <- mean((zip.zeros %>% dplyr::filter(cluster == i))[,j+1])
   }
   average.images[i,257] <- i
}

op <- par(mfrow=c(2,2))
apply(average.images[,-ncol(average.images)],1, plot.zip, use.first = T)
```

### Principal Components

The principal components for this data set are calculated. The scatterplot of the scores in the two first components are plotted, using different colors for points in different clusters. 

```{r}
zip.PC <- princomp(zip.zeros[,-c(1,ncol(zip.zeros))])
df <- data.frame(Comp1 = zip.PC$scores[,1], Comp2 = zip.PC$scores[,2], cluster = factor(zip.zeros$cluster))
df %>% ggplot(aes(x = Comp1, y = Comp2)) + 
   geom_point(aes(color = cluster)) +
   theme_minimal()
```

### For each of the $k = 4$ Clusters, do ...

#### Consider the bivariate data set of the scores PC1 and PC2 of the points in the cluster

```{r}
df.clust1 <- df %>% dplyr::filter(cluster == 1)
df.clust2 <- df %>% dplyr::filter(cluster == 2)
df.clust3 <- df %>% dplyr::filter(cluster == 3)
df.clust4 <- df %>% dplyr::filter(cluster == 4)
```

#### Estimate non-parametrically the joint density of (PC1, PC2) conditional to the cluster, using the default bandwidth values. Represent the estimated bivariate density via the 75% level curve

```{r}
plot(zip.PC$scores[,1:2],pch=19,cex=.5,col=zip.zeros[,ncol(zip.zeros)]+1)
sm.density(df.clust1[, c(1,2)], display="slice",props=c(75), add = T)
sm.density(df.clust2[, c(1,2)], display="slice",props=c(75), add = T)
sm.density(df.clust3[, c(1,2)], display="slice",props=c(75), add = T)
sm.density(df.clust4[, c(1,2)], display="slice",props=c(75), add = T)
legend(x = "bottomright", legend = c("1","2","3","4"), title = "Cluster",
       col=c(2,3,4,5), pch = 20, cex = 0.7)
```

### Principal Curve

The principal curve obtained from the 256-dimensional set of zeros will be represented in the previous plot, using the package **princurve**. The principal curve is plotted as a dotted line below. 

```{r}
plot(zip.PC$scores[,1:2],pch=19,cex=.5,col=zip.zeros[,ncol(zip.zeros)]+1)
sm.density(df.clust1[, c(1,2)], display="slice",props=c(75), add = T)
sm.density(df.clust2[, c(1,2)], display="slice",props=c(75), add = T)
sm.density(df.clust3[, c(1,2)], display="slice",props=c(75), add = T)
sm.density(df.clust4[, c(1,2)], display="slice",props=c(75), add = T)
legend(x = "bottomright", legend = c("1","2","3","4"), title = "Cluster",
       col=c(2,3,4,5), pch = 20, cex = 0.7)
princ.curve <- principal_curve(as.matrix(zip.zeros[,-c(1,ncol(zip.zeros))]))
# Projecting the principal curve over the Principal Components
proj.prcv.to.PC <- predict(zip.PC,princ.curve$s)
lines(proj.prcv.to.PC[princ.curve$ord,1:2],col=1, lty = 2, lwd=2)
```

### For each of the $k = 4$ Clusters, do ...

#### Consider the univariate data set of the 'lambda' scores over the principal curve of the points in this cluster.

```{r}
# Make a new data set with lambda and clusters. 
df2 <- data.frame(lambda = princ.curve$lambda, cluster = factor(zip.zeros$cluster))

# Make new data frames (from the above) for each cluster. 
df2.clust1 <- df2 %>% dplyr::filter(cluster == 1)
df2.clust2 <- df2 %>% dplyr::filter(cluster == 2)
df2.clust3 <- df2 %>% dplyr::filter(cluster == 3)
df2.clust4 <- df2 %>% dplyr::filter(cluster == 4)
```

#### Estimate non-parametrically the density function of lambda, conditional to this cluster. Use the default bandwith value. Plot the estimated density function.

```{r}
plot(zip.PC$scores[,1:2],pch=19,cex=.5,col=zip.zeros[,ncol(zip.zeros)]+1)
lines(proj.prcv.to.PC[princ.curve$ord,1:2],col=1, lty = 2, lwd=2)
legend(x = "bottomright", legend = c("1","2","3","4"), title = "Cluster",
       col=c(2,3,4,5), pch = 20, cex = 0.7)

op <- par(mfrow=c(2,2))
sm.density(df2.clust1[, 1], add = F, col = "2")
sm.density(df2.clust2[, 1], add = F, col = "3")
sm.density(df2.clust3[, 1], add = F, col = "4")
sm.density(df2.clust4[, 1], add = F, col = "5")

par(mfrow=c(1,1))
sm.density.compare(x = df2$lambda, group = df2$cluster, col = c(2,3,4,5),
                   lty=c(1,1,1,1))
legend(x = "right", legend = c("1","2","3","4"), title = "Cluster",
       col=c(2,3,4,5), pch = 20, cex = 0.7)
```

Above the non-parametric density estimation of lambda in each cluster is plotted by itself, and then together using the **sm.density.compare()** function.
In the problem description it sounds like this should be plotted over the principal component plot above, but we do not understand how this could be done.

The density plots shows that in cluster 1 lambdas are lowest, followed by cluster 4, cluster 2, and finally cluster 3. 
This matches with previous plots of the principal curve which clearly goes through the clusters in this order.
Looking at the average zero plot in each group, we observe that the zeros in cluster 1 and 4 are similar (wide). 
In cluster 2 the zeros are smaller and narrower, and in cluster 3 they are the smallest. 
It looks like the principal curve moves from large and wide zeros, to smaller and narrow ones.


 
# Choosing the smoothing parameter in Principal Curves (Hastie and Stuetzle 1989)

Consider the 3-dimensional data generated from the following code. 

```{r}
t <- seq(-1.5*pi,1.5*pi,l=100)
R<- 1
n<-75
sd.eps <- .15
set.seed(1)

y <-  R*sign(t) - R*sign(t)*cos(t/R)
x <- -R*sin(t/R)
z <- (y/(2*R))^2
rt <- sort(runif(n)*3*pi - 1.5*pi)
eps <- rnorm(n)*sd.eps
ry <- R*sign(rt) - (R+eps)*sign(rt)*cos(rt/R)
rx <- -(R+eps)*sin(rt/R)
rz <- (ry/(2*R))^2 + runif(n,min=-2*sd.eps,max=2*sd.eps)
XYZ <- cbind(rx,ry,rz)
require(plot3D)
lines3D(x,y,z,colvar = NULL,
phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
col=2,lwd=4,as=1,
xlim=range(rx),ylim=range(ry),zlim=range(rz))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)
```

## Questions

### Choose the Value of the Degrees of Freedom by Leave-One-Out Cross-Validation (LOOCV)
