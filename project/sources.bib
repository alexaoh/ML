@misc{davies,
  doi = {10.48550/ARXIV.1402.4293},
  url = {https://arxiv.org/abs/1402.4293},
  author = {Davies, Alex and Ghahramani, Zoubin},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Random Forest Kernel and other kernels for big data from random partitions},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cutajar,
  doi = {10.48550/ARXIV.1602.06693},
  url = {https://arxiv.org/abs/1602.06693},
  author = {Cutajar, Kurt and Osborne, Michael A. and Cunningham, John P. and Filippone, Maurizio},
  keywords = {Machine Learning (stat.ML), Computation (stat.CO), Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Preconditioning Kernel Matrices},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{wiki:Matrix-free_methods,
   author = "Wikipedia",
   title = "{Matrix-free methods} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2022",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Matrix-free\%20methods&oldid=1052656767}},
   note = "[Online; accessed 25-May-2022]"
 }
 
 @misc{Ma,
  doi = {10.48550/ARXIV.1712.06559},
  url = {https://arxiv.org/abs/1712.06559},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{rasmussen, 
        place={Cambridge, Mass}, 
        title={Gaussian processes for machine learning}, 
        publisher={MIT Press}, 
        author={Rasmussen, Carl Edward and I., Williams Christopher K}, 
        year={2008}
} 

@misc{belkin,
  doi = {10.48550/ARXIV.1806.06144},
  url = {https://arxiv.org/abs/1806.06144},
  author = {Ma, Siyuan and Belkin, Mikhail},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Kernel machines that adapt to GPUs for effective large batch training},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@MISC{sundaram_acceleratingmachine,
    author = {Narayanan Sundaram and Bryan Catanzaro},
    title = {Accelerating Machine Learning Applications on Graphics Processors},
    year = {}
}

@inproceedings{cotter,
  title={A GPU-tailored approach for training kernelized SVMs},
  author={Andrew Cotter and Nathan Srebro and Joseph Keshet},
  booktitle={KDD},
  year={2011}
}

@article{genton,
author = {Genton, Marc G.},
title = {Classes of Kernels for Machine Learning: A Statistics Perspective},
year = {2002},
issue_date = {3/1/2002},
publisher = {JMLR.org},
volume = {2},
issn = {1532-4435},
abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {299–312},
numpages = {14}
}

@inproceedings{hamers,
author = {Hamers, Bart and Suykens, Johan and De Moor, Bart},
year = {2002},
month = {08},
pages = {720-726},
title = {Compactly Supported RBF Kernels for Sparsifying the Gram Matrix in LS-SVM Regression Models},
volume = {2415},
isbn = {978-3-540-44074-1},
doi = {10.1007/3-540-46084-5_117}
}

@book{quarteroni, 
        place={Berlin},
        title={Numerical mathematics}, 
        publisher={Springer}, 
        author={Quarteroni, Alfio and Sacco, Riccardo and Saleri, Fausto}, 
        year={2010}
} 

@TECHREPORT{shewchuk,
            author = {Jonathan Richard Shewchuk},
            title = {An introduction to the conjugate gradient method without the agonizing pain},
            institution = {},
            year = {1994}
}

@ARTICLE{drineas,
    author = {Petros Drineas and Michael W. Mahoney},
    title = {On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning},
    journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
    year = {2005},
    volume = {6},
    pages = {2005}
}

@MISC{smola,
    author = {Jyrki Kivinen and Alex J. Smola and Robert C. Williamson},
    title = {Learning With Kernels},
    year = {2002}
}


@book{wright,
        title = "Numerical optimization",
        author = "Jorge Nocedal and Wright, {Stephen J.}",
        year = "2006",
        language = "English (US)",
        series = "Springer Series in Operations Research and Financial Engineering",
        publisher = "Springer Nature",
        pages = "1--664",
        booktitle = "Springer Series in Operations Research and Financial Engineering",

}

@MISC{gondzio,
    author = {J. Gondzio},
    title = {Matrix-free interior point method },
    year = {2012}
}

